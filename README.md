
# API Scraping Template

This project is a template for scraping data from APIs using Python. It includes modules for handling API requests, parsing the response data, and saving the results to JSON files. The project uses [Poetry](https://python-poetry.org/) for dependency management and packaging.

## Getting Started

### Prerequisites

- Python 3.7 or higher
- Poetry installed. If you don't have Poetry installed, you can install it via:

```bash
curl -sSL https://install.python-poetry.org | python3 -
```

### Installation

1. Clone the repository:
    ```bash
    git clone git@github.com:adexltd/outreach-scraper.git
    cd outreach-scraper
    ```

2. Install dependencies using Poetry:
    ```bash
    poetry install
    ```
    This will create a virtual environment and install all the required dependencies as specified in `pyproject.toml`.

### Running the Project

1. Activate the virtual environment:
    ```bash
    poetry shell
    ```

2. You can run the main script using:

    ```bash
    python outreach_sheet.py
    ```

Before running, make sure to update the following user-changeable variables in the `main.py` file:

### User-Changeable Variables:

- **save_path**: 
    - Path where the scraped JSON results will be saved.
    - Default: 
    ```python
    save_path = Path('/home/polymorphisma/adex/apollo-scraper-api/archive')
    ```

- **read_path**: 
    - Path to the input CSV file that contains the websites to scrape.
    - Default: 
    ```python
    read_path = r'main.csv'
    ```

- **EMAIL_CREDIT_USED** and **EMAIL_CREDIT_LIMIT**: 
    - These variables control how many email credits are used in scraping emails. If the used credits exceed the limit, the scraper stops.
    - Default: 
    ```python
    EMAIL_CREDIT_USED = 122
    EMAIL_CREDIT_LIMIT = 8000
    ```

Make sure to update these variables according to your needs.

### Project Files

- **_parser.py**: Contains functions for parsing and processing the API responses into a format that can be easily manipulated or saved.
- **apis.py**: This file contains the API configurations including the URL, headers, payload, and HTTP method for each API endpoint.
- **archive/**: A directory to store archived or old files.
- **main.py**: The entry point of the project. It ties everything together by reading API configurations, making requests, handling errors, and saving the results.
- **poetry.lock**: A lock file generated by Poetry to ensure that all dependencies are consistent across environments.
- **pyproject.toml**: The configuration file used by Poetry for dependency management and project metadata.
- **utils.py**: Utility functions that include making HTTP requests, reading/writing JSON files, and other helper methods.

### Error Handling
Errors during scraping are logged in the `error.txt` file with the corresponding URL and error message. If an error occurs while processing a specific URL, it will be recorded in this file.

## Contributing

If you would like to contribute to this project, please fork the repository and submit a pull request. We welcome all contributions!

## Author
[polymorphisma](https://github.com/polymorphisma)
